# X002_Databricks - Module Databricks Lakehouse

## ğŸ“‹ Vue d'ensemble

Ce module implÃ©mente une architecture Medallion (Bronze-Silver-Gold) dans Databricks pour le traitement et l'analyse des donnÃ©es Green IT. Il orchestre la transformation des donnÃ©es brutes issues d'Oracle vers un modÃ¨le en Ã©toile optimisÃ© pour l'analyse BI.

## ğŸ—ï¸ Architecture

### Architecture Medallion (Bronze â†’ Silver â†’ Gold)

L'architecture Medallion est une approche de conception de lakehouse qui organise les donnÃ©es en trois couches progressives :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BRONZE    â”‚â”€â”€â”€â”€â–¶â”‚   SILVER    â”‚â”€â”€â”€â”€â–¶â”‚    GOLD     â”‚
â”‚ DonnÃ©es     â”‚     â”‚  DonnÃ©es    â”‚     â”‚  ModÃ¨le     â”‚
â”‚  Brutes     â”‚     â”‚  NettoyÃ©es  â”‚     â”‚  Analytique â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸŸ¤ Couche Bronze (Raw Data)
- **RÃ´le** : Stockage des donnÃ©es brutes sans transformation
- **Format** : Parquet avec compression Snappy
- **Source** : Exports depuis Oracle Database
- **CaractÃ©ristiques** : Append-only, schÃ©ma source prÃ©servÃ©

#### âšª Couche Silver (Cleaned Data)
- **RÃ´le** : Nettoyage, validation et enrichissement
- **Transformations** :
  - Standardisation des textes (lowercase, trim)
  - Conversion des types de donnÃ©es
  - Gestion des valeurs NULL
  - Calcul d'indicateurs (intensitÃ© carbone, efficacitÃ© Ã©nergÃ©tique)
  - Suppression des doublons
- **QualitÃ©** : Validation des rÃ¨gles mÃ©tier

#### ğŸŸ¡ Couche Gold (Analytics-Ready)
- **RÃ´le** : ModÃ¨le en Ã©toile pour l'analyse BI
- **Structure** : Star Schema optimisÃ©
- **Consommateurs** : Power BI, Tableau, outils analytiques

## ğŸ“ Structure des dossiers et fichiers

### Racine du module
```
X002_Databricks/
â”œâ”€â”€ config.py                    # Configuration centralisÃ©e
â”œâ”€â”€ D_00.5_STRUCTURE.ipynb       # Documentation de la structure
â”œâ”€â”€ README.MD                    # Ce fichier
â”œâ”€â”€ D_01_notebooks/              # Notebooks de traitement
â””â”€â”€ D_02_Medallion/              # Couches de donnÃ©es
```

### ğŸ“„ Fichiers principaux

#### `config.py`
**RÃ´le** : Fichier de configuration centralisÃ©e pour tous les notebooks
- DÃ©finit les chemins des couches Bronze, Silver et Gold
- Configure les chemins des fichiers Parquet
- GÃ¨re les mÃ©tadonnÃ©es du projet (propriÃ©taire, environnement)
- Facilite la maintenance et la rÃ©utilisation du code

**Importance** : Centralise la configuration pour Ã©viter la duplication et simplifier les migrations d'environnement.

#### `D_00.5_STRUCTURE.ipynb`
**RÃ´le** : Notebook de documentation et setup de la structure
- GÃ©nÃ¨re automatiquement la structure des dossiers
- CrÃ©e les fichiers README pour chaque couche
- Documente l'architecture complÃ¨te
- GÃ©nÃ¨re le fichier de configuration

**Importance** : Automatise la crÃ©ation et la documentation de l'infrastructure du projet.

### ğŸ““ D_01_notebooks/ - Notebooks de traitement

#### `01_bronze_ingestion.ipynb`
**Objectif** : Ingestion des donnÃ©es brutes depuis Oracle vers la couche Bronze

**Processus** :
1. Charge les donnÃ©es Parquet exportÃ©es depuis Oracle
2. Valide la structure des donnÃ©es source
3. Sauvegarde en format Parquet dans la couche Bronze
4. Conserve le schÃ©ma original sans modification

**Technologies** :
- PySpark pour le traitement distribuÃ©
- Format Parquet pour performances optimales

#### `02_silver_transformation.ipynb`
**Objectif** : Transformation et nettoyage des donnÃ©es Bronze vers Silver

**Processus** :
1. **Nettoyage** :
   - Standardisation des textes (lowercase, trim)
   - Conversion des types de donnÃ©es
   - Gestion des NULL et valeurs aberrantes

2. **Enrichissement** :
   - Calcul de l'intensitÃ© carbone (Carbon_Footprint_kg / Energy_Consumption_MWh)
   - CatÃ©gorisation de l'efficacitÃ© Ã©nergÃ©tique
   - Flag pour sources d'Ã©nergie renouvelable
   - Ajout de mÃ©tadonnÃ©es (timestamp de traitement)

3. **Validation** :
   - Suppression des doublons
   - VÃ©rification des rÃ¨gles mÃ©tier
   - ContrÃ´les de qualitÃ©

**Technologies** :
- PySpark SQL pour transformations complexes
- UDFs (User Defined Functions) pour logique mÃ©tier

#### `03_gold_star_schema.ipynb`
**Objectif** : CrÃ©ation du modÃ¨le en Ã©toile (Star Schema) pour l'analyse BI

**Processus** :
1. **CrÃ©ation des tables de dimension** :
   - DIM_WORKLOAD : Types de workload et caractÃ©ristiques
   - DIM_ENERGY : Sources d'Ã©nergie et part renouvelable
   - DIM_SECURITY : Niveaux de sÃ©curitÃ© et statut PQC
   - DIM_SCENARIO : ScÃ©narios de workload

2. **CrÃ©ation de la table de faits** :
   - FACT_GREEN_WORKLOAD : MÃ©triques principales
   - ClÃ©s Ã©trangÃ¨res vers les dimensions
   - Mesures : Ã‰nergie, Carbone, CoÃ»t, Performance

3. **Optimisations** :
   - GÃ©nÃ©ration de clÃ©s de substitution (surrogate keys)
   - Relations Fact-Dimension
   - Partitionnement pour performances

**Technologies** :
- ModÃ©lisation dimensionnelle (Kimball)
- PySpark pour gÃ©nÃ©ration de clÃ©s et jointures

### ğŸ“Š D_02_Medallion/ - Couches de donnÃ©es

#### M_01_Bronze/
**Contenu** : DonnÃ©es brutes
- `green_workload_bronze.parquet` : Dataset principal non transformÃ©
- `README.md` : Documentation de la couche Bronze

**CaractÃ©ristiques** :
- Format : Parquet avec compression Snappy
- Mise Ã  jour : Append-only (historisation complÃ¨te)
- SchÃ©ma : Identique Ã  la source

#### M_02_Silver/
**Contenu** : DonnÃ©es nettoyÃ©es et enrichies
- `green_workload_silver.parquet` : Dataset transformÃ©
- `README.md` : Documentation des transformations

**CaractÃ©ristiques** :
- DonnÃ©es validÃ©es et standardisÃ©es
- Colonnes calculÃ©es ajoutÃ©es
- PrÃªt pour analyse exploratoire

#### M_03_Gold/
**Contenu** : ModÃ¨le en Ã©toile
- `DIM_WORKLOAD.parquet` : Dimension Workload
- `DIM_ENERGY.parquet` : Dimension Energy
- `DIM_SECURITY.parquet` : Dimension Security
- `DIM_SCENARIO.parquet` : Dimension Scenario
- `FACT_GREEN_WORKLOAD.parquet` : Table de faits principale
- `README.md` : Documentation du Star Schema

**CaractÃ©ristiques** :
- OptimisÃ© pour requÃªtes BI
- Relations Fact-Dimension Ã©tablies
- PrÃªt pour connexion Power BI/Tableau

## ğŸ”§ Technologies utilisÃ©es et leur importance

### 1. **Databricks**
**Importance** : 
- Plateforme unifiÃ©e pour le lakehouse analytique
- Gestion collaborative des notebooks
- IntÃ©gration native avec Spark
- ScalabilitÃ© horizontale automatique

**Usage** : Environnement principal d'exÃ©cution et de dÃ©veloppement

### 2. **Apache Spark (PySpark)**
**Importance** :
- Traitement distribuÃ© de grandes volumÃ©tries de donnÃ©es
- Performance optimale pour transformations complexes
- API Python intuitive (PySpark DataFrame)
- Support natif du format Parquet

**Usage** : Moteur de traitement pour toutes les transformations Bronzeâ†’Silverâ†’Gold

### 3. **Format Parquet**
**Importance** :
- Format colonnaire optimisÃ© pour l'analyse
- Compression efficace (rÃ©duction de 70-90% vs CSV)
- Support des schÃ©mas complexes et imbriquÃ©s
- Lecture sÃ©lective des colonnes (predicate pushdown)

**Usage** : Format de stockage pour toutes les couches (Bronze, Silver, Gold)

### 4. **Architecture Medallion**
**Importance** :
- SÃ©paration claire des responsabilitÃ©s (SoC)
- TraÃ§abilitÃ© et auditabilitÃ© des transformations
- PossibilitÃ© de retraitement Ã  chaque couche
- Optimisation progressive de la qualitÃ© des donnÃ©es

**Usage** : Patron architectural du module

### 5. **Star Schema (ModÃ©lisation Kimball)**
**Importance** :
- Performance optimale pour requÃªtes BI
- SimplicitÃ© pour utilisateurs mÃ©tier
- DÃ©normalisation contrÃ´lÃ©e
- Facilite les agrÃ©gations et le drill-down

**Usage** : ModÃ¨le de donnÃ©es de la couche Gold

## ğŸ”„ Flux de donnÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     GREEN-IT-DATA-PLATFORM                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. EXTRACTION (Oracle)
   X001_Oracle/O_03_LOAD_CSV_DATA/
   â”‚
   â”œâ”€â–¶ ETL Oracle (SP_ETL_LOAD_STAR_SCHEMA.sql)
   â””â”€â–¶ Export Parquet (export_to_parquet.py)
        â”‚
        â””â”€â–¶ green_it_metrics.parquet
             â”‚
             â–¼

2. BRONZE INGESTION (Databricks)
   01_bronze_ingestion.ipynb
   â”‚
   â””â”€â–¶ M_01_Bronze/green_workload_bronze.parquet
        â”‚
        â–¼

3. SILVER TRANSFORMATION (Databricks)
   02_silver_transformation.ipynb
   â”‚
   â”œâ”€â–¶ Cleaning (standardisation, type conversion)
   â”œâ”€â–¶ Enrichment (carbon intensity, efficiency flags)
   â””â”€â–¶ Validation (duplicates, quality checks)
        â”‚
        â””â”€â–¶ M_02_Silver/green_workload_silver.parquet
             â”‚
             â–¼

4. GOLD MODELING (Databricks)
   03_gold_star_schema.ipynb
   â”‚
   â”œâ”€â–¶ Dimensions (Workload, Energy, Security, Scenario)
   â”œâ”€â–¶ Fact (Green Workload metrics)
   â””â”€â–¶ M_03_Gold/
        â”œâ”€ DIM_WORKLOAD.parquet
        â”œâ”€ DIM_ENERGY.parquet
        â”œâ”€ DIM_SECURITY.parquet
        â”œâ”€ DIM_SCENARIO.parquet
        â””â”€ FACT_GREEN_WORKLOAD.parquet
             â”‚
             â–¼

5. VISUALISATION (Power BI)
   X003_Powerbi/
   â”‚
   â””â”€â–¶ Dashboards et rapports Green IT
```

## ğŸš€ Utilisation

### PrÃ©requis
- Workspace Databricks configurÃ©
- AccÃ¨s aux donnÃ©es exportÃ©es depuis Oracle
- Python 3.8+ avec PySpark

### ExÃ©cution sÃ©quentielle

1. **Configuration**
   ```python
   # VÃ©rifier config.py et ajuster les chemins si nÃ©cessaire
   ```

2. **Bronze - Ingestion**
   ```python
   # ExÃ©cuter 01_bronze_ingestion.ipynb
   # Charge les donnÃ©es brutes depuis Oracle exports
   ```

3. **Silver - Transformation**
   ```python
   # ExÃ©cuter 02_silver_transformation.ipynb
   # Nettoie et enrichit les donnÃ©es
   ```

4. **Gold - ModÃ©lisation**
   ```python
   # ExÃ©cuter 03_gold_star_schema.ipynb
   # CrÃ©e le modÃ¨le en Ã©toile pour BI
   ```

### VÃ©rification
Chaque notebook inclut des cellules de validation :
- Comptage des enregistrements
- Affichage d'Ã©chantillons
- VÃ©rification des schÃ©mas
- Statistiques descriptives

## ğŸ“Š ModÃ¨le de donnÃ©es Gold (Star Schema)

### Tables de dimension

#### DIM_WORKLOAD
- Workload_ID (PK)
- Workload_Type
- Workload_Scenario
- Workload_Strategy

#### DIM_ENERGY
- Energy_ID (PK)
- Energy_Source
- Renewable_Share_Percent

#### DIM_SECURITY
- Security_ID (PK)
- Security_Level
- PQC_Status

#### DIM_SCENARIO
- Scenario_ID (PK)
- Workload_Scenario
- Scenario_Description

### Table de faits

#### FACT_GREEN_WORKLOAD
- Fact_ID (PK)
- Workload_ID (FK)
- Energy_ID (FK)
- Security_ID (FK)
- Scenario_ID (FK)
- **MÃ©triques** :
  - Energy_Consumption_MWh
  - Carbon_Footprint_kg
  - Cost_per_Unit
  - Performance_TPM
  - Carbon_Intensity

## ğŸ” SÃ©curitÃ© et gouvernance

- **DonnÃ©es sensibles** : Aucune donnÃ©e personnelle stockÃ©e
- **AccÃ¨s** : ContrÃ´lÃ© via Databricks Workspace permissions
- **Audit** : Logs d'exÃ©cution des notebooks
- **Versioning** : Historique des modifications via Git

## ğŸ“ˆ Avantages de l'architecture

1. **ScalabilitÃ©** : Architecture distribuÃ©e Spark
2. **Performance** : Format Parquet colonnaire et compression
3. **QualitÃ©** : Validation progressive Ã  chaque couche
4. **MaintenabilitÃ©** : SÃ©paration claire des responsabilitÃ©s
5. **TraÃ§abilitÃ©** : Historisation complÃ¨te (Bronze)
6. **FlexibilitÃ©** : Retraitement possible Ã  chaque niveau
7. **BI-Ready** : ModÃ¨le en Ã©toile optimisÃ© pour analyses

## ğŸ”— IntÃ©grations

- **En amont** : X001_Oracle (source de donnÃ©es)
- **En aval** : X003_Powerbi (visualisation)
- **ParallÃ¨le** : X002_lakehouse (architecture alternative)

## ğŸ“ Notes

- Les notebooks sont conÃ§us pour Ãªtre exÃ©cutÃ©s dans l'ordre (01 â†’ 02 â†’ 03)
- La configuration centralisÃ©e facilite la migration entre environnements
- L'architecture Medallion permet le retraitement incrÃ©mental si nÃ©cessaire
- Le modÃ¨le en Ã©toile est optimisÃ© pour les requÃªtes BI typiques

## ğŸ¯ Objectif final

Fournir une plateforme analytique performante et Ã©volutive pour l'analyse des mÃ©triques Green IT, permettant :
- La visualisation des consommations Ã©nergÃ©tiques
- L'analyse de l'empreinte carbone
- L'optimisation des coÃ»ts
- L'Ã©valuation de l'efficacitÃ© des workloads

---

**Projet** : GREEN-IT-DATA-PLATFORM  
**Module** : X002_Databricks  
**Architecture** : Medallion (Bronze-Silver-Gold)  
**Environnement** : Databricks Lakehouse  
