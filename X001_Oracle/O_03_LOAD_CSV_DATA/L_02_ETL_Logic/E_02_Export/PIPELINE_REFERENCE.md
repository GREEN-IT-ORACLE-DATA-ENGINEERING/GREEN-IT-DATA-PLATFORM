# ğŸŒ± Green IT Data Platform

## Automated Oracle â†’ Parquet ETL Pipeline

Ce dÃ©pÃ´t implÃ©mente un **pipeline ETL industriel** dÃ©diÃ© Ã  la collecte, la structuration et lâ€™archivage des mÃ©triques **Green IT** issues dâ€™une base de donnÃ©es **Oracle 19c**.

Lâ€™objectif principal est de fournir des jeux de donnÃ©es **optimisÃ©s pour lâ€™analytics et les plateformes Cloud / Lakehouse (Spark, Databricks)**, tout en garantissant :

- la **traÃ§abilitÃ©** des donnÃ©es,
- lâ€™**automatisation complÃ¨te** du pipeline,
- la **conformitÃ© aux bonnes pratiques** du Data Engineering.

---

## ğŸ¯ Objectifs du Pipeline

- Extraire les mÃ©triques Green IT depuis un **Star Schema Oracle**
- Transformer les donnÃ©es en **format Parquet compressÃ©**
- Versionner automatiquement les exports dans **GitHub**
- PrÃ©parer les donnÃ©es pour une consommation **Big Data / Lakehouse**

---

## ğŸ—ï¸ Architecture

La structure respecte une sÃ©paration claire entre **logique mÃ©tier**, **orchestration** et **donnÃ©es finales**.

### ğŸ“ Description des composants

- **`export_to_parquet.py`**
  Script Python ETL chargÃ© de :
  - lâ€™extraction des donnÃ©es Oracle,
  - les jointures SQL,
  - la transformation,
  - lâ€™export des donnÃ©es en fichiers Parquet.

- **`sync_to_github.sh`**
  Script Bash dâ€™orchestration assurant :
  - lâ€™exÃ©cution du pipeline ETL,
  - la gestion des erreurs,
  - lâ€™automatisation Git (commit et push).

- **`E_03_Data_Output/`**
  RÃ©pertoire contenant exclusivement les **fichiers Parquet gÃ©nÃ©rÃ©s**, prÃªts pour lâ€™analyse.

---

## âš™ï¸ DÃ©tails Techniques

### 1ï¸âƒ£ Extraction & Transformation (Python)

Le script `export_to_parquet.py` rÃ©alise les opÃ©rations suivantes :

- Connexion sÃ©curisÃ©e Ã  **Oracle 19c**
- Jointure de la table de faits :
  - `FACT_GREEN_WORKLOAD`

- Avec les dimensions :
  - `DIM_WORKLOAD`
  - `DIM_ENERGY`
  - `DIM_SECURITY`
  - `DIM_SCENARIO`

- Chargement des donnÃ©es via **Pandas**
- Export en **Parquet (pyarrow)** avec :
  - compression `snappy`,
  - format optimisÃ© pour **Spark / Databricks**.

---

### 2ï¸âƒ£ Orchestration & Versioning (Bash)

Le script `sync_to_github.sh` automatise lâ€™ensemble du processus :

- ExÃ©cution du script Python ETL
- VÃ©rification du code de retour (sÃ©curitÃ© dâ€™exÃ©cution)
- Synchronisation Git ciblÃ©e :
  - `git add` uniquement sur les fichiers Parquet
  - `git commit` horodatÃ©
  - `git push` vers la branche dÃ©diÃ©e **`data-pipeline-branch`**

âœ… Cette approche garantit que **le code reste stable** et que **seules les donnÃ©es sont versionnÃ©es**.

---

## ğŸ” SÃ©curitÃ© & Bonnes Pratiques

- Credentials Oracle stockÃ©s dans des **variables dâ€™environnement**
- Aucun mot de passe prÃ©sent dans le code
- Utilisation de **chemins absolus** pour Ã©viter les erreurs dâ€™environnement
- Branche Git dÃ©diÃ©e aux exports (sÃ©paration stricte **Code / Data**)

---

## ğŸš€ ExÃ©cution du Pipeline

### â–¶ï¸ ExÃ©cution Manuelle

```bash
./X001_Oracle/O_03_LOAD_CSV_DATA/L_02_ETL_Logic/E_02_Export/sync_to_github.sh
```

---

## ğŸ¤– Automatisation et Orchestration (Crontab)

Afin de garantir la **fraÃ®cheur des donnÃ©es** et dâ€™Ã©liminer toute intervention manuelle, le pipeline Green IT est entiÃ¨rement **automatisÃ© via Crontab** sous Linux.

Cette automatisation transforme un traitement ponctuel en un **Data Flow autonome, fiable et reproductible**, conforme aux standards industriels du Data Engineering.

---

## ğŸ› ï¸ Commandes dâ€™ExÃ©cution et de Validation

### 1ï¸âƒ£ Attribution des droits dâ€™exÃ©cution

```bash
chmod +x /home/oracle/green_it/X001_Oracle/O_03_LOAD_CSV_DATA/L_02_ETL_Logic/E_02_Export/sync_to_github.sh
```

---

### 2ï¸âƒ£ Configuration de la tÃ¢che Crontab

AccÃ¨s Ã  lâ€™Ã©diteur Crontab :

```bash
crontab -e
```

Planification du pipeline (exÃ©cution **le 1er de chaque mois Ã  00h00**) :

```bash
00 00 1 * * /home/oracle/green_it/X001_Oracle/O_03_LOAD_CSV_DATA/L_02_ETL_Logic/E_02_Export/sync_to_github.sh >> /home/oracle/green_it/etl_cron_log.log 2>&1
```

ğŸ“„ **Gestion des logs** :

- Toutes les sorties standards et erreurs sont redirigÃ©es vers `etl_cron_log.log`
- Permet un **monitoring prÃ©cis** et une traÃ§abilitÃ© complÃ¨te des exÃ©cutions

---

## ğŸ¯ Conclusion

Ce projet illustre la mise en place rÃ©ussie dâ€™une **infrastructure de donnÃ©es End-to-End performante et industrialisÃ©e**.

En transformant des donnÃ©es complexes issues dâ€™**Oracle 19c** en fichiers **Parquet versionnÃ©s**, cette architecture :

- garantit **fiabilitÃ©, intÃ©gritÃ© et disponibilitÃ©** des mÃ©triques Green IT,
- fournit une base solide pour lâ€™**analyse dÃ©cisionnelle**,
- facilite le **reporting environnemental**,
- et sâ€™intÃ¨gre naturellement dans une architecture **Lakehouse / Big Data**.
